1. Download the latest ROR release to ROR directory (for example):

  mv ~/Downloads/v1.1-2022-06-16-ror-data.json .

2. Link to ror-data.json:

  rm ror-data.json
  ln -s v1.1-2022-06-16-ror-data.json ror-data.json

3. Generate ror_metadata.csv file from json:

  python3 extract_ror_metadata.py 

4. Get current Wikidata ROR statements (generates wikidata_ror.csv):

  python3 fetch_wikidata_ror_entries.py 

5. Compare ROR metadata and Wikidata ROR entries (generates ror_wd_diffs.csv):

  python3 comparisons.py 

6. Create new ROR release in Wikidata - see Q111395396 for example

7. Update qs_add_ror.py with new ROR release ID

8. Filter ror_wd_diffs to get ROR ID's with Wikidata links not found in WD:
     grep "missing in" ror_wd_diffs.csv > unlinked_vxx.csv
     (and clean up that file copied to ror_wikidata.csv)

9. Run quickstatements generated by python script to add these links to WD:
  python3 qs_add_ror.py > qs_tmp

10. Filter ror_wd_diffs to get ROR ID's with mismatches:
     grep "differs" ror_wd_diffs.csv > mismatches_vxx
    
11. Then compare with previous mismatches, looking at only new ones:
     sort mismatches_v1.0 > tmp
     sort mismatches_v1.1 > tmp2
     diff tmp tmp2 | grep '>' > tmp3
    and look at the two qids to see if something needs to be fixed

12. Re-run #4 above and then run 
     python3 ror_md_for_or.py
   to generate ror_or_metadata.csv for use with OpenRefine

13.
   (a) For each, filter those with no wikidata qid
   (b) Check for ones with a wikipedia URL, if specific enough, link up!
      -- if a lot, dump list to match and use map_wp_wd.rb to automate getting wikidata id's from wikipedia URL's, then qs_dump.rb to get quickstatements format
   (c) Match purely on name, see how it does...
   (d) Match on name and country
   (e) Match on name and url
   (f) Match on name and ISNI ?
   (g) for each, export csv of matches to run APSbot on

(14) From "create" subdirectory, run APSbot_ror_create on remaining entries, after updating the release qid in the ror_release_qid file
